# Unit 6: Further topics on random variables

## The Difference of Two Independent Exponential Random Variables

* Convolution
* X - Yçš„å–å€¼å¯èƒ½æ˜¯æ­£çš„ä¹Ÿå¯èƒ½æ˜¯è´Ÿçš„ï¼Œæ‰€ä»¥$\displaystyle Z = X - Y$éœ€è¦åˆ†æƒ…å†µè®¨è®ºã€‚
* $\displaystyle P(X = 1) = p(-X = -1) \to f_{-Y}(Z - X) = f_Y(X - Z)$
* åˆ©ç”¨å¯¹ç§°æ€§ç®€åŒ–è®¡ç®—ã€‚
* å¦ä¸€ç§æ€è·¯å°±æ˜¯ç›´æ¥è®¡ç®—æ‰€æœ‰å¯èƒ½çš„ç‚¹çš„æ¦‚ç‡ï¼Œåˆ©ç”¨joint PDFã€‚

## The Sum of Discrete and Continuous Random Variables

Let X be a discrete random variable with PMF pX and let Y be a continuous random variable,independent from X, with PDF $f_Y$. Derive a formula for the PDF of the random variable X +Y. 

* ä½¿ç”¨äº†total probability theorm
* åŸºæœ¬ç­–ç•¥ï¼šå¯¹CDFæ±‚å¯¼$\displaystyle \to$ PDF
* è®¡ç®—$\displaystyle \sum_X P_X(x)$,å¦‚æœXå–ä¸€ä¸ªæ— é™å€¼ï¼Œåˆ™éœ€è¦å¦å¤–çš„å‡è®¾ï¼Œææ–™ä¸­æ²¡æœ‰ç»™å‡ºã€‚
* æœ¬é¢˜ç­‰æ•ˆäºå·ç§¯çš„è®¡ç®—å…¬å¼æ¨å¯¼ã€‚

## Law of Iterated

* $E[X \mid Y] = g(y)$
æˆ‘ä»¬ä¹‹æ‰€ä»¥å°†$E[X \mid Y] = g(y)$ï¼Œæ˜¯å› ä¸ºE[]æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¸¸é‡ã€‚ç»“åˆlect12ä¸­çš„ä¾‹å­å¯ä»¥çŸ¥é“ï¼Œyæœ¬è´¨ä¸Šæ˜¯æˆ‘ä»¬åˆ†ç»„çš„æ ‡å‡†ï¼Œå½“ä¸ªyæŸä¸€ä¸ªå€¼æ—¶ï¼Œæˆ‘ä»¬å¯¹åº”çš„å°†æŒ‰ç…§è¿™ä¸ªå€¼å–å‡ºäº†ä¸€éƒ¨åˆ†çš„Xï¼Œç„¶åæ±‚è¿™ä¸€éƒ¨åˆ†xçš„æœŸæœ›ã€‚è¿™ä¹Ÿæ˜¯æ¡ä»¶å˜é‡åœ¨ç°å®ä¸­çš„åº”ç”¨ï¼Œåœ¨å¤§å¤šæ•°çš„åœºæ™¯ä¸‹ï¼Œå®ƒå¯¹åº”ç€åˆ†ç»„ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°† $E[X \mid Y]$è§†ä½œæ˜¯å…³äºyçš„å¸¸é‡ã€‚

$
\begin{aligned}
E[E[X \mid Y]] 
&= \sum_Y g(Y)P_Y(y) \\ 
&= \sum_Y  \sum_X xP(X \mid Y)P_Y(y) \\
&= \sum_X x \sum_Y P(X \mid Y)P_Y(y) \xrightarrow[total\quad probability]{} \sum_X xP(x) \\
&= E[X]  
\end{aligned}
$  

* $var(X) = E [var(X|Y)] + var (E[X|Y])$
è¿™ä¸ªè¡¨è¾¾å¼:law of total variance

$ var(X) = E[X^2]-E[X]^2$
$ var(X \mid Y) = E[X^2 \mid Y]-E[X \mid Y]^2$
$var(E[X \mid Y]) = E[E[X \mid Y]^2]-E[E[X \mid Y]]^2$

$\displaystyle \begin{aligned}
  E[var(X \mid Y)] 
  &= E[E[X^2 \mid Y]-E[X \mid Y]^2]  \\
  &= E[X^2] - E[E[X \mid Y]^2]
\end{aligned}$

ç”±ä¸¤ä¸ªè¡¨è¾¾å¼åŠ å’Œå¯ä»¥æ¨å¯¼å‡º law of total variance

* $E[X]$æ˜¯ä¸€ä¸ªå¸¸é‡
* $E[X \mid Y]$æ˜¯ä¸€ä¸ªä¸€ä¸ªå…³äºYçš„å˜é‡ã€‚$\displaystyle E[X \mid Y = 1] $ä»–ç­‰æ•ˆäºg(Y) $\displaystyle g(1)$ã€‚å› ä¸ºYæ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œæ‰€ä»¥$\displaystyle g(Y)$ä¹Ÿæ˜¯ä¸€ä¸ªéšæœºå˜é‡ã€‚

## The Variance in the Stick Breaking Problem

law of iterated expectations and law of total variance å¸®åŠ©ä½ è®¡ç®—E[X]å’Œvar(X)ã€‚é€šè¿‡ä½¿ç”¨conditionçš„æ€æƒ³æ˜¯çš„å°†Xçš„æœŸæœ›å’Œæ–¹å·®èƒ½å¤Ÿè½è„šåœ¨å·²çŸ¥æƒ…å†µä¹‹ä¸Šï¼Œåˆ†é˜¶æ®µè®¡ç®—ã€‚
ä¸€ä¸ªé‡è¦çš„ç‚¹æ˜¯ï¼šéœ€è¦ç€é‡è€ƒè™‘æ¡ä»¶Yï¼Œå› ä¸ºå¦‚æœYçš„é€‰å–ç»“æœä¸å½“ï¼Œå¯¹è§£å†³é—®é¢˜ä¼šæ²¡æœ‰å¸®åŠ©ï¼Œè¿™ä¹Ÿæ˜¯éœ€è¦é€šè¿‡ç»ƒä¹ ç€é‡è®­ç»ƒçš„ç‚¹ã€‚

ä¸€ä¸ªè®¡ç®—æŠ€å·§ï¼š

$var(X) = E[X^2] - (E[X])^2$ï¼Œåˆ©ç”¨è¿™ä¸ªè¡¨è¾¾å¼å¯ä»¥ç®€ä¾¿æ±‚E[X^2]

## Widgets and Crates

è¿™é“é¢˜çš„ç‰¹ç‚¹æ˜¯ï¼šä»–æœ‰éå¸¸æ¸…æ™°çš„å±‚æ¬¡ï¼Œå› æ­¤å¾ˆå®¹æ˜“èƒ½å¤Ÿä»åº•å±‚å‘ä¸Šå±‚æ„å»ºæ¡ä»¶æ¥ç®€åŒ–è®¡ç®—ã€‚

æ³¨æ„ç‚¹ï¼šT= $\displaystyle \sum_1^N X_i$ Næ˜¯éšæœºå˜é‡ã€‚

## Using the Conditional Expectation and Variance

The random variables X and Y are described by a joint PDF which is constant within the unit area quadrilateral with vertices (0, 0), (0, 1), (1, 2), and (1, 1). Use the law of total variance to find the variance of X + Y .

* ä½¿ç”¨ total varianceï¼Œæ—¢å¯ä»¥ä¸€Xä½œä¸ºæ¡ä»¶ï¼Œä¹Ÿå¯ä»¥ä»¥Yä½œä¸ºæ¡ä»¶ã€‚
* $E[X + Y \mid X] = E[Y] + X$ï¼Œå°†Xè§†ä½œæ˜¯ä¸€ä¸ªå¸¸é‡(è™½ç„¶ä»–æ˜¯ä¸€ä¸ªå˜é‡ï¼Œä½†æ„ä¹‰å‘ç”Ÿäº†å˜åŒ–ï¼Œå¯¹è¿™ä¸ªå˜é‡ä½œå¸¸æ•°å¤„ç†ï¼š$E[x + Y \mid X = x]$)
* $\displaystyle var(X + Y \mid X) = var(Y)$
* Xå’ŒYçš„è”åˆåˆ†å¸ƒæ˜¯å‡åŒ€åˆ†å¸ƒï¼Œé‚£ä¹ˆæ¡ä»¶åˆ†å¸ƒå¹¶ä¸ä¼šæ”¹å˜è¿™ç§åˆ†å¸ƒï¼Œæ‰€ä»¥Xæˆ–è€…Yçš„åˆ†å¸ƒä¾ç„¶æ˜¯å‡åŒ€åˆ†å¸ƒã€‚

## A Random Number of Coin Flips

(a) You roll a fair six-sided die, and then you flip a fair coin the number of times shown by the
die. Find the expected value and the variance of the number of heads obtained.
(b) Repeat part (a) for the case where you roll two dice, instead of one.

åœ¨è®¡ç®—ç¦»æ•£çš„å‡åŒ€åˆ†å¸ƒæ—¶ï¼Œä¸€ä¸ªç®€ä¾¿çš„è®¡ç®—å…¬å¼ï¼š$ var(N) = \frac{(b-a)(b - a + 2)}{12}$

åœ¨è®¡ç®— béƒ¨åˆ†æ—¶ï¼Œå› ä¸ºæœ‰ä¸¤ä¸ªğŸ²ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡å°†è¿™ä¸ªå®éªŒæ‹†è§£æˆä¸¤ä¸ªå•ç‹¬çš„å®éªŒä»¥æ­¤å¯ä»¥è®©è®¡ç®—ç®€ä¾¿ã€‚


## A Coin with Random Bias

We toss n times a biased coin whose probability of heads, denoted by
q, is the value of a random variable Q with given mean Âµ and positive variance Ïƒ2. Let Xi be a Bernoulli random variable that models the outcome of the ith toss (i.e., Xi =1 if the ith toss is a head). We assume that X1,...,Xn are conditionally independent, given Q = q. Let X be the number of heads obtained in the n tosses.
(a) Use the law of iterated expectations to find E[Xi] and E[X].
(b) Find cov(Xi, Xj ). Are X1,...,Xn independent?
(c) Use the law of total variance to find var(X). Verify your answer using the covariance result of part (b). 

* $ var(X) = var(X_1 + X_2 +X_3 + \dots + X_1 ) = var + cov $,ä»–çš„æ¨å¯¼å¯ä»¥è®¾ç½®$E[X_i] = 0$ 
* è®¡ç®—béƒ¨åˆ†æ—¶ï¼Œå¯¹iå’Œjçš„è®¨è®ºéœ€è¦åˆ†æƒ…å†µï¼ši = j or $ i != j $
* å¯¹äºbernoulli X, var(X) = p(1- p), E[X] = p
* è®¡ç®—$var(X) = var(X_1 + X_2 +X_3 + \cdots+ X_n )$æœ‰ä¸¤ç§è®¡ç®—æ€è·¯ï¼Œä¸€ç§æ˜¯åˆ©ç”¨total variance of law , å¦ä¸€ç§æ˜¯ä½¿ç”¨ $ var(X_1 + X_2 +X_3 + \cdots+ X_n ) = var + cov$

## k-sided fair die

Consider n independent tosses of a k-sided fair die. Let Xi be the number of tosses that result in i.
(a) Are X1 and X2 uncorrelated, positively correlated, or negatively correlated? Give a one-line
justification.
(b) Compute the covariance cov(X1, X2) of X1 and X2.

* E[A_tB_s] = 0,if t = s.
å› ä¸ºå½“t=sæ—¶ï¼Œæ„å‘³ç€è¿™æ˜¯åŒä¸€æ¬¡çš„æŠ•æ·ç­›å­ï¼Œä½ å¯ä»¥åˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„ç»“æœï¼Œæœ‰å››ç§ï¼Œä¸‰ç§å¯èƒ½çš„å€¼æ˜¯0,ä¸€ç§å¯èƒ½çš„å€¼ä¸ä¸º0,ä½†æ˜¯å‘ç”Ÿçš„æ¦‚ç‡æ˜¯0.

## total variance 

![](ref/sp6/20231109155601.png)

* åœ¨è®¡ç®— var(X)ï¼Œä½¿ç”¨total variance: var(X) = E[var(X | Y)] + var(E[X | Y])
* åœ¨è®¡ç®— var(E[X | Y])æ—¶ï¼Œç”¨æ–¹å·®çš„å®šä¹‰è®¡ç®—ï¼Œç„¶åä¹˜ä»¥å½“å‰æ¡ä»¶çš„æ¦‚ç‡ï¼š$\frac{2}{10}(0 - \frac{3}{10})^2 + \frac{6}{10}(\frac{1}{2} - \frac{3}{10})^2 +\frac{2}{10}(0 - \frac{3}{10})^2$
* ä»¥Xæˆ–è€…Yä½œä¸ºæ¡ä»¶æ—¶ï¼Œè¿™ä¸ªæ¡ä»¶æ»¡è¶³çš„æ¦‚ç‡åœ¨å‡åŒ€åˆ†å¸ƒä¸‹æŒ‡çš„æ˜¯é¢ç§¯è€Œä¸æ˜¯é•¿åº¦ï¼š$\displaystyle 2 \leq y \leq 1$ï¼Œè¿™ä¸ªæ¡ä»¶çš„æ¦‚ç‡æ˜¯ $\displaystyle \frac{2}{10}$
* æ¡ä»¶æ–¹å·®ä¹Ÿå¯ä»¥ä½¿ç”¨å®šä¹‰å¼æ¥è®¡ç®—ï¼š$\displaystyle E[(Y - E[Y \mid X])^2 \mid X] $
* æ³¨æ„è¿ç”¨å¯¹ç§°å±æ€§ï¼šå¯¹äºYï¼Œå¦‚æœå…³äºXè½´å¯¹ç§°ï¼Œæ±‚ç§¯åˆ†ä¸ºå¶æ•°æ–¹ï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥ç®€åŒ–è¿ç®—ã€‚

## wombat club  

The wombat club has N members, where N is a random variable with PMF
pN (n) = p nâˆ’1(1 âˆ’ p) for n = 1, 2, 3, . . ..
On the second Tuesday night of every month, the club holds a meeting. Each wombat member
attends the meeting with probability q, independently of all the other members. If a wombat
attends the meeting, then it brings an amount of money, M, which is a continuous random
variable with PDF
fM(m) = Î»eâˆ’Î»m for m â‰¥ 0.

N, M, and whether each wombat member attends are all independent. Determine:

(a) The expectation and variance of the number of wombats showing up to the meeting.
(b) The expectation and variance for the total amount of money brought to the meeting.


## G1

(a) Let X1, X2, . . . , Xn, Xn+1, . . . , X2n be independent and identically distributed random variables.
Find
E[X1 | X1 + X2 + . . . + Xn = x0],
where x0 is a constant.
(b) Define
Sk = X1 + X2 + . . . + Xk, 1 â‰¤ k â‰¤ 2n.
Find
E[X1 | Sn = sn, Sn+1 = sn+1, . . . , S2n = s2n],
where sn, sn+1, . . . , s2n are constants